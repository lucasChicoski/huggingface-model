{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chicoski/src/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, GPT2LMHeadModel, GPT2Tokenizer,BertForQuestionAnswering, BertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small', padding_side='left') \n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDataEnglish = \"The solar system consists of the Sun and all celestial objects orbiting it. There are eight major planets in our solar system, with Earth being the only one known to support life. The largest planet is Jupiter, while the smallest is Mercury. Beyond the planets, there are numerous asteroids, comets, and dwarf planets, such as Pluto. The study of our solar system continues to reveal its vast complexity and beauty.\"\n",
    "# myDataEnglish = \"asd, aasddsds\"\n",
    "batch_size = 2\n",
    "num_epochs = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data.split(',')\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "dataset = CustomDataSet(myDataEnglish)\n",
    "dataLoader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Loss: 8.95949935913086\n",
      "Epoch [2/5] Loss: 7.261989593505859\n",
      "Epoch [3/5] Loss: 4.7879791259765625\n",
      "Epoch [4/5] Loss: 2.95976185798645\n",
      "Epoch [5/5] Loss: 9.934455871582031\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for bach in dataLoader:\n",
    "        idsTraining = tokenizer(bach, return_tensors='pt', padding=True, truncation=True)\n",
    "        input_ids = idsTraining['input_ids']\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # print(loss)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataLoader)\n",
    "    # print(avg_loss)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chicoski/src/homebrew/lib/python3.11/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/Users/chicoski/src/homebrew/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10919,  9151,   750,   262,  5259, 26277, 14595,   287,  5633, 50256,\n",
      "            13,  1318,   318,   691,   530,  1900,  1688,  5440,   284,  1104,\n",
      "          1204,    13, 22409,    13,   383,  4387,   318, 22721,    13],\n",
      "        [10919,  9151,   750,   262,  5259, 26277, 14595,   287,  5633, 50256,\n",
      "            13,  1318,   318,   691,   530,  1900,  1688,  5440,   284,  1104,\n",
      "          1204,    13, 22409,    13,  1318,   389,    13,  1318,   389]])\n",
      "what ocean did the titanic sink in?<|endoftext|>. There is only one known major planet to support life. intercourse. The largest is Jupiter.\n",
      "what ocean did the titanic sink in?<|endoftext|>. There is only one known major planet to support life. intercourse. There are. There are\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([\"what ocean did the titanic sink in ?\"], return_tensors=\"pt\")\n",
    "\n",
    "# What are the major components of our solar system?\n",
    "# print(inputs)\n",
    "# what ocean did the titanic sink in ?\n",
    "# inputs = tokenizer('Qaundo o titanic afundou ?', return_tensors=\"pt\")\n",
    "# outputs = model(asd['input_ids'])\n",
    "# print(inputs['input_ids'])\n",
    "outputs = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_new_tokens=20,\n",
    "    num_beams=2,\n",
    "    num_return_sequences=2,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "\n",
    "    # max_length=50,  # Comprimento máximo do texto gerado\n",
    "    # min_length=10,  # Comprimento mínimo do texto gerado\n",
    "    # num_return_sequences=1,  # Três sequências geradas\n",
    "    temperature=10,  # Temperatura moderada para aleatoriedade\n",
    "    # top_k=50,  # Limita as escolhas de token\n",
    "    # top_p=0.9,  # Controla a proporção acumulada de probabilidade\n",
    "    # repetition_penalty=1.2,  # Penalização de repetição de tokens\n",
    "    # pad_token_id=tokenizer.pad_token_id,  # Token de preenchimento\n",
    "    # eos_token_id=tokenizer.eos_token_id,  # Token de fim de sequência\n",
    "    eos_token_id=4013,  # Token de fim de sequência\n",
    "    # length_penalty=0.1\n",
    ")\n",
    "\n",
    "\n",
    "# print(outputs)\n",
    "\n",
    "output_ids = outputs['sequences']\n",
    "# output_ids = outputs[0]\n",
    "print(output_ids)\n",
    "# print(len(output_ids))\n",
    "# resposta = tokenizer.decode(output_ids[0])\n",
    "for id in output_ids:\n",
    "    resposta = tokenizer.decode(id)\n",
    "    print(resposta)\n",
    "# resposta = tokenizer.decode()\n",
    "# print(resposta)\n",
    "\n",
    "# resposte = tokenizer.decode(outputs['sequences'])\n",
    "# print(resposte)\n",
    "# print(outputs['sequences'])sequences_scores, beam_indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
