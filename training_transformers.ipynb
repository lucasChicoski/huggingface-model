{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, GPT2LMHeadModel, GPT2Tokenizer,BertForQuestionAnswering, BertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small', padding_side='left') \n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDataEnglish = \"The solar system consists of the Sun and all celestial objects orbiting it. There are eight major planets in our solar system, with Earth being the only one known to support life. The largest planet is Jupiter, while the smallest is Mercury. Beyond the planets, there are numerous asteroids, comets, and dwarf planets, such as Pluto. The study of our solar system continues to reveal its vast complexity and beauty.\"\n",
    "# myDataEnglish = \"asd, aasddsds\"\n",
    "batch_size = 2\n",
    "num_epochs = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data.split(',')\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "dataset = CustomDataSet(myDataEnglish)\n",
    "dataLoader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Loss: 6.690196514129639\n",
      "Epoch [2/5] Loss: 7.1412272453308105\n",
      "Epoch [3/5] Loss: 9.96776008605957\n",
      "Epoch [4/5] Loss: 8.598865509033203\n",
      "Epoch [5/5] Loss: 1.4640692472457886\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for bach in dataLoader:\n",
    "        idsTraining = tokenizer(bach, return_tensors='pt', padding=True, truncation=True)\n",
    "        input_ids = idsTraining['input_ids']\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # print(loss)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataLoader)\n",
    "    # print(avg_loss)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10919,  9151,   750,   262,  5259, 26277, 14595,   287,  5633,   383,\n",
      "          4387,   318, 21673,    13,  1318,   389,  6409, 14705,   287,   674,\n",
      "          6591,  1080,   290,   389, 14705,    13,   383,  3825,   318],\n",
      "        [10919,  9151,   750,   262,  5259, 26277, 14595,   287,  5633,   383,\n",
      "          4387,   318, 21673,    13,  1318,   389,  6409, 14705,   287,   674,\n",
      "          6591,  1080,   290,   389, 14705,    13, 22721,   318,   530]])\n",
      "what ocean did the titanic sink in? The largest is Mercury. There are numerous planets in our solar system and are planets. The Sun is\n",
      "what ocean did the titanic sink in? The largest is Mercury. There are numerous planets in our solar system and are planets. Jupiter is one\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([\"what ocean did the titanic sink in ?\"], return_tensors=\"pt\")\n",
    "\n",
    "# What are the major components of our solar system?\n",
    "# print(inputs)\n",
    "# what ocean did the titanic sink in ?\n",
    "# inputs = tokenizer('Qaundo o titanic afundou ?', return_tensors=\"pt\")\n",
    "# outputs = model(asd['input_ids'])\n",
    "# print(inputs['input_ids'])\n",
    "outputs = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_new_tokens=20,\n",
    "    num_beams=2,\n",
    "    num_return_sequences=2,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "\n",
    "    # max_length=50,  # Comprimento máximo do texto gerado\n",
    "    # min_length=10,  # Comprimento mínimo do texto gerado\n",
    "    # num_return_sequences=1,  # Três sequências geradas\n",
    "    temperature=10,  # Temperatura moderada para aleatoriedade\n",
    "    # top_k=50,  # Limita as escolhas de token\n",
    "    # top_p=0.9,  # Controla a proporção acumulada de probabilidade\n",
    "    # repetition_penalty=1.2,  # Penalização de repetição de tokens\n",
    "    # pad_token_id=tokenizer.pad_token_id,  # Token de preenchimento\n",
    "    # eos_token_id=tokenizer.eos_token_id,  # Token de fim de sequência\n",
    "    eos_token_id=4013,  # Token de fim de sequência\n",
    "    # length_penalty=0.1\n",
    ")\n",
    "\n",
    "\n",
    "# print(outputs)\n",
    "\n",
    "output_ids = outputs['sequences']\n",
    "# output_ids = outputs[0]\n",
    "print(output_ids)\n",
    "# print(len(output_ids))\n",
    "# resposta = tokenizer.decode(output_ids[0])\n",
    "for id in output_ids:\n",
    "    resposta = tokenizer.decode(id)\n",
    "    print(resposta)\n",
    "# resposta = tokenizer.decode()\n",
    "# print(resposta)\n",
    "\n",
    "# resposte = tokenizer.decode(outputs['sequences'])\n",
    "# print(resposte)\n",
    "# print(outputs['sequences'])sequences_scores, beam_indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
